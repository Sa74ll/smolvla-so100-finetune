{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sa74ll/ELM_challenge/blob/main/02_eval_offline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGJ9JIk8d5-2"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/huggingface/lerobot.git\n",
        "%cd /content/lerobot\n",
        "!pip install -e .\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "i-emuQWZpoC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/lerobot\n",
        "!pip install -e \".[smolvla]\""
      ],
      "metadata": {
        "id": "27DbExP-p-wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "\n",
        "from lerobot.datasets.lerobot_dataset import LeRobotDataset, LeRobotDatasetMetadata\n",
        "from lerobot.policies.smolvla.modeling_smolvla import SmolVLAPolicy\n",
        "from lerobot.policies.factory import make_pre_post_processors\n",
        "\n",
        "# CONFIG\n",
        "DATASET_REPO = \"lerobot/svla_so101_pickplace\"\n",
        "BATCH_SIZE = 24\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "BQsm1mD6fIuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load policy"
      ],
      "metadata": {
        "id": "DzedoBNawDVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load policy from Hugging face + dataset stats\n",
        "\n",
        "policy = SmolVLAPolicy.from_pretrained(\"Sa74ll/smolvla_so101_color_aug_best_model1\") #load best policy that saved in HF\n",
        "policy.to(DEVICE).eval()\n",
        "\n",
        "meta = LeRobotDatasetMetadata(DATASET_REPO)\n",
        "preprocessor, _ = make_pre_post_processors(policy.config, dataset_stats=meta.stats)\n",
        "\n",
        "fps = meta.fps\n",
        "chunk_size = policy.config.chunk_size\n",
        "print(\"chunk_size:\", chunk_size)\n",
        "print(\"fps:\", fps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZMT-H_FfI8U",
        "outputId": "d0b57555-8839-49eb-ac2d-1f5c4e371fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading  HuggingFaceTB/SmolVLM2-500M-Video-Instruct weights ...\n",
            "Reducing the number of VLM layers to 16 ...\n",
            "chunk_size: 50\n",
            "fps: 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action_stats = meta.stats[\"action\"]\n",
        "action_min = torch.tensor(action_stats[\"min\"])\n",
        "action_max = torch.tensor(action_stats[\"max\"])\n",
        "action_mean = torch.tensor(action_stats[\"mean\"])\n",
        "action_std  = torch.tensor(action_stats[\"std\"])\n"
      ],
      "metadata": {
        "id": "i2G5dRPtfJXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "delta_timestamps = {              # build timestamps to match model\n",
        "    \"observation.state\": [0.0],\n",
        "    \"observation.images.up\": [0.0],\n",
        "    \"observation.images.side\": [0.0],\n",
        "    \"action\": [i / fps for i in range(chunk_size)], #SmolVLA predicts 50 actions (chunk_size), 50 action timestamps from the dataset to match model shape by dividing each action into frame per second (FPS)\n",
        "}"
      ],
      "metadata": {
        "id": "tnYro--0fJlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build val split (episodes >= 40)\n",
        "base_ds = LeRobotDataset(DATASET_REPO, video_backend=\"pyav\") #load dateset first without delta_timestamps to get the episode indices\n",
        "episode_idx = np.array(base_ds.hf_dataset[\"episode_index\"])\n",
        "val_indices = [i for i, ep in enumerate(episode_idx) if ep >= 40]\n",
        "\n",
        "val_full = LeRobotDataset(\n",
        "    DATASET_REPO,\n",
        "    delta_timestamps=delta_timestamps,\n",
        "    video_backend=\"pyav\",\n",
        ")\n",
        "val_ds = Subset(val_full, val_indices)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "print(f\"val samples: {len(val_ds)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwAYvDu6f6KH",
        "outputId": "fe39ae4c-f6aa-42a9-a45b-0635b729c260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val samples: 2759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helpers\n",
        "def fix_keys(batch):\n",
        "    \"\"\" remap camers name \"\"\"\n",
        "    if \"observation.images.up\" in batch:\n",
        "        batch[\"observation.images.camera1\"] = batch.pop(\"observation.images.up\")\n",
        "    if \"observation.images.side\" in batch:\n",
        "        batch[\"observation.images.camera2\"] = batch.pop(\"observation.images.side\")\n",
        "    return batch\n",
        "\n",
        "def unnormalize_pred(pred_norm: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Model outputs normalized actions (because training loss was on normalized), so map back to dataset space using mean/std from meta.stats\"\"\"\n",
        "    return pred_norm * action_std + action_mean\n",
        "\n"
      ],
      "metadata": {
        "id": "QM_6QsXGf_ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eval\n",
        "all_preds_raw = []\n",
        "all_gts_raw = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, raw in enumerate(val_loader):\n",
        "        \"\"\" ground truth in RAW space (dataset space)\"\"\"\n",
        "        gt_raw = raw[\"action\"][:, 0, :].clone()     # instead of getting all the 50 action (B,T,6), we just get the first action (B,6)\n",
        "\n",
        "        raw = fix_keys(raw)\n",
        "        batch = preprocessor(raw)\n",
        "\n",
        "        for k, v in list(batch.items()):\n",
        "            if torch.is_tensor(v):\n",
        "                batch[k] = v.to(DEVICE)\n",
        "\n",
        "        # model inference to normalised action sequence\n",
        "        pred_seq = policy.predict_action_chunk(batch)    # (B, 50, 6)\n",
        "        pred_step0 = pred_seq[:, 0, :].cpu()             # (B,6) normalized\n",
        "\n",
        "        # bring prediction back to RAW space\n",
        "        pred_raw = unnormalize_pred(pred_step0)          # (B,6)\n",
        "        # append the preds and gts to lists\n",
        "        all_preds_raw.append(pred_raw)\n",
        "        all_gts_raw.append(gt_raw)\n",
        "\n",
        "\n",
        "all_preds_raw = torch.cat(all_preds_raw, dim=0)\n",
        "all_gts_raw   = torch.cat(all_gts_raw, dim=0)\n",
        "\n",
        "print(\"Collected preds:\", all_preds_raw.shape, \"Ground truths:\", all_gts_raw.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe5kSV1_gBH0",
        "outputId": "769da3e3-bb00-4bb8-a2c0-fe795ae94110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected preds: torch.Size([2759, 6]) Ground truths: torch.Size([2759, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics: per-joint 5% of its own range\n",
        "\n",
        "joint_ranges = action_max - action_min\n",
        "tol = joint_ranges * 0.05     # 5% per joint\n",
        "\n",
        "abs_err = torch.abs(all_preds_raw - all_gts_raw)   # (N,6)\n",
        "within_5pr = abs_err <= tol                       # (True/False split)\n",
        "\n",
        "per_joint_success = within_5pr.float().mean(dim=0) * 100.0\n",
        "overall_mean = per_joint_success.mean().item()\n",
        "\n",
        "print(\"\\n========== EVAL (per-joint 5%) ==========\")\n",
        "for j, s in enumerate(per_joint_success):\n",
        "    print(f\"joint {j}: {s:.2f}%\")\n",
        "\n",
        "print(f\"\\nAverage per-joint success (5%): {overall_mean:.2f}%\")\n",
        "print(\"=========================================\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCG_B8x3gIcT",
        "outputId": "cf8d6f21-bd39-48b7-b0cb-f54f2c012f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== EVAL (per-joint 5%) ==========\n",
            "joint 0: 45.81%\n",
            "joint 1: 47.88%\n",
            "joint 2: 70.06%\n",
            "joint 3: 77.46%\n",
            "joint 4: 60.86%\n",
            "joint 5: 63.43%\n",
            "\n",
            "Average per-joint success (5%): 60.92%\n",
            "=========================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
